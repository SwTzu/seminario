                

\section{Marco Conceptual}
\label{sc:MC}


Las heridas cr'onicas del pie diab'etico presentan cicatrizaci'on prolongada y alto riesgo de complicaciones. Se clasifican por gravedad mediante escalas como Wagner o PWAT y evolucionan a trav'es de fases inflamatoria, proliferativa y de remodelaci'on, afectadas por la multimorbilidad propia de estos pacientes. Pese a estos sistemas, la evaluaci'on sigue siendo manual y dependiente del especialista, lo que motiva aplicar visi'on por computador y aprendizaje autom'atico para cuantificar la lesi'on de forma objetiva.



\section{Estado del Arte}
\label{sc:EA}


Las heridas crónicas, como las úlceras diabéticas del pie , representan un serio problema de salud mundial por su alta prevalencia y complicaciones. Se estima que entre un 19\% y 34\% de los pacientes diabéticos desarrollarán una úlcera de pie en su vida, con elevado riesgo de mala cicatrización, amputación de miembros inferiores e incluso reducción de la supervivencia \cite{Zhang2022}. Las úlceras por presión (escaras) y las úlceras venosas son otros tipos comunes de heridas crónicas, todas contribuyendo a costos sanitarios multimillonarios \cite{Wang2020,Sendilraj2024}. La evaluación clínica de estas lesiones suele basarse en la inspección visual y mediciones manuales, procesos sujetos a la experiencia del profesional y, por tanto, con variabilidad interobservador \cite{Curti2024}. Para estandarizar la valoración, se han propuesto escalas clínicas como \textbf{BWAT}, \textbf{PUSH} o la clasificación de \textbf{Wagner}, entre otras. Sin embargo, aplicarlas requiere tiempo y puede ser subjetivo. En la última década, la \textbf{inteligencia artificial} (IA) – especialmente técnicas de \textbf{machine learning} (ML) y \textbf{deep learning} (DL) – ha emergido como herramienta prometedora para automatizar la evaluación de heridas ulcerosas, buscando mayor objetividad, eficiencia y seguimiento continuo \cite{Sendilraj2024,Curti2024}.

A continuación, se presenta una revisión crítica de trabajos relevantes (2015–2025) sobre el uso de IA en la evaluación automática de heridas ulcerosas, con énfasis en pie diabético pero sin limitarse a este. Se abordan desarrollos en segmentación de imágenes (delimitación de la herida y análisis de tamaño), clasificación y gradación de severidad, análisis de tejidos en la herida, predicción de evolución de la cicatrización, así como enfoques multimodales. Se comparan metodologías, resultados, conjuntos de datos empleados, fortalezas, debilidades y posibles mejoras, organizando la información en secciones temáticas para facilitar su lectura.

\subsection{Escalas clínicas de evaluación de heridas}

En la práctica clínica, existen escalas estandarizadas para evaluar el estado y la evolución de heridas. Por ejemplo, \textbf{BWAT (Bates-Jensen Wound Assessment Tool)} consta de 13 ítems que evalúan características como:
\begin{enumerate}
    \item tamaño
    \item profundidad de la herida
    \item bordes
    \item presencia de socavamiento
    \item tipo de tejido necrotico
    \item cantidad de tejido necrotico
    \item cantidad de tejido de ganulacion
    \item cantidad de tejido de epilizacion
    \item tipo de exudado
    \item cantidad de exudad
    \item condicion de la piel peilesional - color
    \item condicion de la piel peilesional - edema
    \item condicion de la piel peilesional - induracion
    
\end{enumerate}
Cada ítem se puntúa de 1 (mejor situación) a 5 (peor), ofreciendo un puntaje total que cuantifica la gravedad de la herida. Debido a que varios ítems requieren examen directo (ej., medir profundidad o socavamientos manualmente), el \textbf{BWAT} tradicionalmente se aplica \textbf{en persona} durante la visita clínica . Esto dificulta su automatización posterior a partir de fotografías, pues algunas características no son discernibles solo con imágenes. Para solventar esa limitación, se introdujo la escala \textbf{PWAT} \cite{Curti2024}. 

El PWAT es una adaptación del BWAT que incluye solo un subconjunto de los ítems –aquellos inferibles directamente de una fotografía–, excluyendo por ejemplo la profundidad o socavamientos. Pese a ser más limitado, el PWAT ha demostrado \textbf{validez y robustez} en aplicaciones clínicas para seguimiento mediante imágenes . Otra escala común es la \textbf{PUSH (Pressure Ulcer Scale for Healing)}, diseñada por NPIAP para monitorizar la cicatrización de úlceras por presión, combinando medidas de área superficial de la herida, cantidad de exudado y tipo de tejido dominante en el lecho ulceroso (granulación, esfacelo/fibrina o epitelial). La PUSH genera un puntaje total que disminuye a medida que la herida cura, facilitando evaluar si una úlcera por presión está mejorando o no

En cuanto a clasificación de severidad, para las úlceras de pie diabético se emplea la \textbf{clasificación de Wagner}, de 6 niveles (0 a 5) que describen la profundidad y complicaciones de la lesión . Un Wagner 0 indica pie de riesgo sin ulceración; Wagner 1 es una úlcera superficial; Wagner 2 implica extensión a tendón, hueso o cápsula articular; Wagner 3 añade infección grave (ej. osteomielitis o absceso); Wagner 4 denota gangrena localizada (por isquemia/infección) en antepié o talón; y Wagner 5 es gangrena extensa de todo el pie, generalmente requiriendo amputación \cite{Girmaw2025}. Esta clasificación guía el manejo clínico del pie diabético y pronostica la necesidad de intervenciones mayores.

Aunque las escalas estandarizadas han reducido parcialmente la subjetividad, sigue habiendo variabilidad entre observadores al aplicarlas. Por ejemplo, dos médicos podrían discrepar en la estimación del porcentaje de tejido de granulación vs. esfacelo en un lecho ulceroso, afectando la puntuación BWAT. En efecto, la evaluación en forma de escalas Likert impone categorías que \textbf{fuerzan la cuantificación} de rasgos de la herida, pero la interpretación visual sigue siendo del clínico . Estudios reportan que persiste una variabilidad inter e intraobservador no desdeñable incluso utilizando herramientas estandarizadas . Esto ha motivado la investigación en sistemas automatizados que extraigan de imágenes métricas objetivas de la herida (dimensiones, tipos de tejido, etc.) para calcular o apoyar estas escalas. A continuación se revisan los avances en dichos sistemas basados en IA.

Segmentar una herida en una imagen consiste en delinear automáticamente el contorno de la lesión y separarla de la piel sana circundante. Es un paso fundamental, pues permite calcular la superficie de la úlcera, un parámetro clave para monitorear la cicatrización (una reducción del área a lo largo del tiempo suele indicar progreso) \cite{Wang2020}. La medición manual del área (ej., con reglas o calcos sobre la herida) es engorrosa, imprecisa y consume tiempo \cite{Filko2023}. Automatizar la segmentación permite obtener \textbf{medidas objetivas de forma rápida}, facilitando la documentación y seguimiento en historias clínicas electrónicas . 

\begin{itemize}
    \item \textbf{Enfoques tradicionales}
    \begin{itemize}
        \item Antes del auge del deep learning, se exploraron métodos de visión por computador clásica para segmentar heridas crónicas. Estos incluían técnicas como umbralización de color, detección de bordes, crecimiento de regiones y uso de clasificadores basados en características diseñadas manualmente. Por ejemplo, Wang \cite{Chemello2022} propusieron un método de dos etapas: primero se sobresegmenta la imagen en superpixels y se extraen múltiples descriptores de color y textura; luego, un conjunto de clasificadores SVM distingue superpixels de herida vs. piel . Este clasificador en cascada logró sensibilidad $\sim73.3\%$ y especificidad $\sim94.6\%$ en segmentar úlceras diabéticas (100 imágenes de 15 pacientes) , superando a enfoques de una sola etapa o redes neuronales poco profundas probadas en esa cohorte. Posteriormente, Wang \cite{Chemello2022}  abordaron el reto de la variabilidad en condiciones de captura (iluminación, ángulo, fondo) entrenando un modelo de Random Field jerárquico (AHRF) que extendía los Conditional Random Fields. Probado en un conjunto pequeño de imágenes sintéticas y reales, este modelo logró especificidad >95\% y sensibilidad >77\% , mostrando mejor desempeño que CRFs tradicionales. Los autores sugirieron que en escenarios con muy pocos datos, modelos probabilísticos como AHRF podrían superar a CNNs profundas, aunque anticiparon que con más datos las redes neuronales lograrían resultados superiores .
    \end{itemize}
    \item \textbf{Deep learning en segmentación}
    \begin{itemize}
        \item A medida que aumentaron los conjuntos de datos de heridas, los métodos de deep learning demostraron ventajas claras en segmentación. Diversos arquitecturas de segmentación semántica han sido aplicadas: Fully Convolutional Networks, U-Net, SegNet, etc. Ohura evaluaron redes para segmentar úlceras combinando datos de distintos tipos de heridas: entrenaron CNNs (SegNet, LinkNet, U-Net) principalmente con 400 imágenes de úlceras por presión y apenas 20 imágenes de UPD \cite{Chemello2022}. Sorprendentemente, la U-Net logró altísima precisión al segmentar incluso las úlceras diabéticas (especificidad 0.943 y sensibilidad 0.993 en promedio) , evidenciando que patrones aprendidos en úlceras por presión podían transferirse a otras etiologías crónicas con morfologías parecidas. Este resultado sugirió que, dadas suficientes imágenes de entrenamiento, las redes profundas segmentan heridas con fiabilidad casi humana
        
        \item Varios grupos han construido sus propios datasets para entrenar modelos. Wang \cite{Wang2020} compilaron 1109 imágenes de úlceras de pie diabético de 889 pacientes y entrenaron un modelo basado en MobileNetV2 para segmentación automática . Eligiendo MobileNetV2 por su ligereza, demostraron que un modelo de baja complejidad podía lograr un rendimiento equiparable a redes más profundas en esa tarea . La arquitectura propuesta combinaba la segmentación por CNN con un post-procesamiento de componentes conexos para afinar la máscara final . Gracias a su menor costo computacional, este enfoque apuntaba a implementaciones en dispositivos móviles sin sacrificar precisión. De hecho, otras investigaciones también resaltan la factibilidad de ejecutar modelos de segmentación en smartphones: Ramachandram \cite{Ramachandram2022} entrenaron modelos U-Net con el mayor conjunto publicado hasta la fecha ($\approx465$ mil pares de imagen-máscara para segmentar heridas, más 17 mil para segmentar tejidos dentro de la herida) obtenidos de la base de datos de la compañía Swift Medical . El modelo resultante segmenta en tiempo casi real en un teléfono, dada la optimización lograda con ese enorme dataset . En pruebas, su red alcanzó un IoU promedio de 0,8644 delimitando el área de la herida (muy alto acuerdo con el contorno verdadero). Incluso en condiciones de iluminación y piel variadas, la segmentación automática mostró ser robusta y sin sesgos por tono de piel – un aspecto importante para equidad clínica

        \item La precisión de la segmentación profunda se refleja también en desafíos internacionales. En la competencia Diabetic Foot Ulcer Challenge (DFUC2020), múltiples equipos aplicaron detectores y segmentadores basados en YOLO, Faster R-CNN y U-Net. Un resumen de Yap et al. reportó que las mejores refinaciones de YOLOv3 lograron $\sim 91.95\%$ de exactitud en detección de úlcera en imagen completa, y variantes de Faster R-CNN alcanzaron hasta 91.4\% mAP . Para segmentación semántica, arquitecturas tipo U-Net destacaron; en un estudio se informa que un modelo U-Net superó a otras arquitecturas con 94.96\% de precisión en segmentación de la herida . Asimismo, aplicando Mask RCNN (que combina detección y segmentación a nivel de instancia) se han logrado valores de precision $\sim 0.86$ y mAP $\sim 0.51$ segmentando úlceras . En suma, la comunidad ha validado que las CNN bien entrenadas pueden delimitar las heridas con alta fiabilidad, permitiendo calcular el área de forma automática y consistente
    \end{itemize}
    \item \textbf{Comparativa de enfoques de segmentación}
    \begin{itemize}
        \item Los métodos basados en DL superan a los clásicos en exactitud, especialmente cuando hay suficiente volumen de datos para entrenar. Los enfoques tradicionales (p. ej. SVM con atributos diseñados) alcanzaron especificidades altas en conjuntos pequeños , pero su sensibilidad quedaba limitada posiblemente por la variabilidad visual que no capturaban totalmente. Las CNN, al aprender características de bajo a alto nivel directamente de los píxeles, manejan mejor dicha variabilidad, logrando sensibilidades y especificidades cercanas al 90–99\% \cite{Chemello2022}. No obstante, un desafío común es la \textbf{generalización}: muchos estudios emplearon datos de un único centro o condiciones controladas, y un modelo puede perder precisión si se aplica a imágenes con distinta iluminación, dispositivos o poblaciones. Para mitigar esto, algunos autores integran pasos de pre-procesamiento de color (ej. conversión a espacios de color uniformes Lab, YCbCr) antes de la segmentación, como en la plataforma DFUCare \cite{Sendilraj2024}, o aplican extensas técnicas de \textit{data augmentation } \cite{Aldughayfiq2023} para expandir la diversidad de ejemplos de entrenamiento. Otra tendencia es combinar modalidades: Filko \cite{Filko2023} desarrollaron un sistema robótico que captura simultáneamente la herida en 2D (fotografía RGB) y en 3D mediante escaneo láser, usando una CNN 2D para segmentar inicialmente la herida en la imagen y luego refinando el contorno sobre la malla 3D con un modelo activo de contornos \cite{Filko2023}. Este enfoque híbrido produce un modelo 3D de la superficie herida, permitiendo medir \textbf{perímetro, área y volumen} de la úlcera de forma totalmente automática . La incorporación de volumen es valiosa, ya que una reducción volumétrica puede indicar curación antes que la reducción de área en ciertas lesiones profundas.
    \end{itemize}
\end{itemize}

\subsection{Clasificación de heridas: detección, severidad e infección}

Otra línea de aplicación de la IA en heridas es la clasificación automática, que puede tomar varias formas según el objetivo clínico:
\begin{itemize}
    \item \textbf{Detección vs. piel sana}
    \begin{itemize}
        \item Identificar si en una foto de un pie o de la piel hay una úlcera presente (clasificación binaria herida/no herida)
    \end{itemize}
    \item \textbf{Clasificación del tipo de herida}
    \begin{itemize}
        \item  Por etiología (diabética, venosa, por presión, quirúrgica, etc.) o por estadio (p. ej. estadios I–IV de úlcera por presión, grados Wagner 0–5 en pie diabético).
    \end{itemize}
    \item \textbf{Detección de signos clínicos en la herida}
    \begin{itemize}
        \item clasificar si una úlcera de pie diabético muestra signos de infección, de isquemia, ambas o ninguna (clasificación multinomial)
    \end{itemize}
    \item \textbf{Gradación de severidad}
    \begin{itemize}
        \item Asociada a escalas como Wagner, Texas, NPUAP, etc., a partir de la imagen.
    \end{itemize}
\end{itemize}

Varios trabajos han explorado redes neuronales para estas tareas de clasificación a nivel de imagen o de herida. En general, las arquitecturas de deep learning utilizadas son CNNs de clasificación (VGG, ResNet, EfficientNet, MobileNet, etc.) o detectores de objetos cuando se necesita localizar la herida en la foto además de clasificarla

\begin{itemize}
    \item \textbf{Detección de úlceras (binario sí/no)}
    \begin{itemize}
        \item Goyal \cite{Goyal2020} desarrollaron \textbf{DFUNet}, una arquitectura de CNN especializada en identificar regiones con úlcera vs piel normal en el pie diabético . DFUNet combinaba convoluciones profundas con capas paralelas y de distinta profundidad para captar características a múltiples escalas, logrando mejorar la discriminación entre piel sana y lesionada . Este trabajo pionero ya demostraba la viabilidad de deep learning para detectar automáticamente una úlcera en imágenes donde pudiera haber artefactos o variabilidad en los pies de pacientes diabéticos. Estudios posteriores han reportado precisiones muy altas en esta tarea: Cassidy \cite{Cassidy2023} hicieron un \textbf{estudio clínico multicéntrico} con 203 fotografías de pies tomadas con un smartphone de bajo costo, pasando cada imagen por un sistema de IA para detectar úlceras y comparando con la evaluación de especialistas. El modelo alcanzó \textbf{91.6\% de sensibilidad y 92.4\% de especificidad} en la detección de úlceras , prácticamente equiparable a la precisión humana. Es notable que incluso con imágenes capturadas en entornos reales (no de estudio), la IA mantuvo alto desempeño, lo cual avala su potencial uso como herramienta de tamizaje remoto. Los clínicos participantes mostraron además alto acuerdo (Kappa >0.8) en que el sistema identifica correctamente las úlceras . Este es uno de los primeros ensayos que prueban un algoritmo de visión computacional en campo con pacientes, marcando un hito hacia la adopción como dispositivo médico

        \item En escenarios más controlados, se han conseguido incluso accuracies cercanas a la perfección en detección binaria, aunque en conjuntos de datos limitados. Por ejemplo, Girmaw y Taye \cite{Girmaw2025} entrenaron un modelo MobileNetV2 para detectar la presencia de UPD y reportan \textbf{100\% de exactitud} en sus pruebas . Si bien este resultado es llamativo, es probable que su dataset no haya sido muy amplio ni diverso, lo que podría indicar cierto \textit{overfitting}. Aun así, refleja la capacidad de las CNN modernas: un modelo ligero fue suficiente para separar completamente imágenes con vs sin úlcera en el conjunto evaluado . Los autores destacan la importancia de la cuidadosa preparación de datos (anotaciones, aumentos, etc.) y ajuste de hiperparámetros para lograr tal rendimiento , subrayando que la clave está tanto en la arquitectura como en el \textit{fine-tuning} adecuado.
    \end{itemize}
        
\end{itemize}

